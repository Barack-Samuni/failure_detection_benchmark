{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "config_name = \"medmnist/pathmnist_resnet_dropout_all_layers.yml\"\n",
    "scores_filename = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "root = \"/data/failure_detection\"\n",
    "sys.path.append(root)\n",
    "from configs.default_config import load_yaml_training_config\n",
    "from failure_detection.evaluator import ThresholdBasedEvaluator\n",
    "from failure_detection.run_evaluation import get_all_scores\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = \"/data/failure_detection\"\n",
    "config = load_yaml_training_config(Path(root) / \"configs\" / config_name)\n",
    "\n",
    "if not Path(scores_filename).exists():\n",
    "    print(\"Score dataframe not created yet, running scores script\")\n",
    "    scores_df = get_all_scores(config_name)\n",
    "else:\n",
    "    scores_df = pd.read_csv(scores_filename)\n",
    "\n",
    "if config.n_classes > 2:\n",
    "    scores_df[\"mcmc_probas\"] = None\n",
    "    scores_df[\"Probas\"] = None\n",
    "    scores_df[\"Laplace_probas\"] = None\n",
    "    scores_df[\"SWAG_probas\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A look at the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    scores_df[\"Targets\"].astype(int).values,\n",
    "    scores_df[\"Predictions\"].astype(int).values,\n",
    "    normalize=\"true\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\n",
    "    f'Acc: {accuracy_score(scores_df[\"Targets\"].astype(int).values, scores_df[\"Predictions\"].astype(int).values):.3f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.n_classes == 2:\n",
    "    CalibrationDisplay.from_predictions(\n",
    "        scores_df[\"Targets\"].values, scores_df[\"Probas\"].values, n_bins=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.n_classes == 2:\n",
    "    sns.histplot(x=scores_df[\"Probas\"], hue=scores_df[\"Targets\"], multiple=\"stack\")\n",
    "    plt.title(\"Distribution of p(y=1) by target (on test set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Softmax baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_baseline = ThresholdBasedEvaluator(\n",
    "    scores_df[\"Baseline\"],\n",
    "    scores_df[\"Predictions\"],\n",
    "    scores_df[\"Targets\"],\n",
    "    scores_df[\"Probas\"],\n",
    "    \"Baseline\",\n",
    ")\n",
    "all_metrics = evaluator_baseline.get_new_metrics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOCTOR results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"doctor_alpha\" in scores_df.columns:\n",
    "    evaluator_alpha = ThresholdBasedEvaluator(\n",
    "        scores_df[\"doctor_alpha\"],\n",
    "        scores_df[\"Predictions\"],\n",
    "        scores_df[\"Targets\"],\n",
    "        scores_df[\"Probas\"],\n",
    "        \"DoctorAlpha\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_alpha.get_new_metrics(), ignore_index=True\n",
    "    )\n",
    "\n",
    "if \"doctor_alpha_pbb\" in scores_df.columns:\n",
    "    evaluator_alpha_pbb = ThresholdBasedEvaluator(\n",
    "        scores_df[\"doctor_alpha_pbb\"],\n",
    "        scores_df[\"Predictions\"],\n",
    "        scores_df[\"Targets\"],\n",
    "        scores_df[\"Probas\"],\n",
    "        \"DoctorAlphaPBB\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_alpha_pbb.get_new_metrics(), ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get TrustScore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TrustScore\" in scores_df.columns:\n",
    "    evaluator_trustscore = ThresholdBasedEvaluator(\n",
    "        scores_df[\"TrustScore\"],\n",
    "        scores_df[\"Predictions\"],\n",
    "        scores_df[\"Targets\"],\n",
    "        scores_df[\"Probas\"],\n",
    "        \"Trust Score\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_trustscore.get_new_metrics(), ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mcmc_soft_scores\" in scores_df.columns:\n",
    "    evaluator_mcmcmean_agg = ThresholdBasedEvaluator(\n",
    "        scores_df[\"mcmc_soft_scores\"],\n",
    "        scores_df[\"mcmc_predictions\"],\n",
    "        scores_df[\"Targets\"],\n",
    "        scores_df[\"mcmc_probas\"],\n",
    "        \"MCMC Average Softmax score - Agg pred\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_mcmcmean_agg.get_new_metrics(), ignore_index=True\n",
    "    )\n",
    "    evaluator_mcmcmean_agg = ThresholdBasedEvaluator(\n",
    "        scores_df[\"mcmc_entropy_scores\"],\n",
    "        scores_df[\"mcmc_predictions\"],\n",
    "        scores_df[\"Targets\"],\n",
    "        scores_df[\"mcmc_probas\"],\n",
    "        \"MCMC Entropy score - Agg pred\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_mcmcmean_agg.get_new_metrics(), ignore_index=True\n",
    "    )\n",
    "    all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Laplace_predictions\" in scores_df.columns:\n",
    "    evaluator_laplace = ThresholdBasedEvaluator(\n",
    "        scores_df[\"Laplace_score\"],\n",
    "        scores_df[\"Laplace_predictions\"],\n",
    "        scores_df[\"Laplace_targets\"],\n",
    "        scores_df[\"Laplace_probas\"],\n",
    "        \"Laplace\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_laplace.get_new_metrics(), ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConfidNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ConfidNet_scores\" in scores_df.columns:\n",
    "    evaluator_confidNet = ThresholdBasedEvaluator(\n",
    "        scores_df[\"ConfidNet_scores\"],\n",
    "        scores_df[\"Predictions\"],\n",
    "        scores_df[\"Targets\"],\n",
    "        scores_df[\"Probas\"],\n",
    "        \"ConfidNet\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_confidNet.get_new_metrics(), ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SWAG_score\" in scores_df.columns:\n",
    "    evaluator_swag = ThresholdBasedEvaluator(\n",
    "        scores_df[\"SWAG_score\"],\n",
    "        scores_df[\"SWAG_predictions\"],\n",
    "        scores_df[\"SWAG_targets\"],\n",
    "        scores_df[\"SWAG_probas\"],\n",
    "        \"SWAG\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(\n",
    "        evaluator_swag.get_new_metrics(), ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DUQ_score\" in scores_df.columns:\n",
    "    evaluator_duq = ThresholdBasedEvaluator(\n",
    "        scores_df[\"DUQ_score\"],\n",
    "        scores_df[\"DUQ_predictions\"],\n",
    "        scores_df[\"Targets\"],\n",
    "        scores_df[\"DUQ_probas\"],\n",
    "        \"DUQ\",\n",
    "    )\n",
    "    all_metrics = all_metrics.append(evaluator_duq.get_new_metrics(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.to_csv(Path(scores_filename).parent / f\"all_metrics.csv\", index=False)\n",
    "all_metrics"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "18efa2ea88a9a0f10d6e5cc4837f7bdf7b7858c8420811408b1cd4ad9b1bf20a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('DeepLearning': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
